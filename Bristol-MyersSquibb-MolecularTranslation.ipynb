{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport cv2\nimport pickle\nimport csv\nimport math\nimport re\nfrom Levenshtein import distance as levenshtein_distance\nimport transformers\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch import Tensor\nimport torch.nn.init as init\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\nfrom torch import optim\nfrom torch.optim.lr_scheduler import MultiStepLR, CyclicLR, ExponentialLR, StepLR\nimport torch.nn.functional as F\nfrom torch.nn import TransformerDecoder, TransformerDecoderLayer\nfrom torchvision import transforms\nfrom matplotlib import pyplot as plt\n!pip install efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet\nimport efficientnet_pytorch\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.7.0.tar.gz (20 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (1.19.5)\nBuilding wheels for collected packages: efficientnet-pytorch\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-py3-none-any.whl size=16033 sha256=8c3ef0d681f3d86811ff0638166fb4b05a89b9fb66c5d1837034f2c9ed7d8740\n  Stored in directory: /root/.cache/pip/wheels/b7/cc/0d/41d384b0071c6f46e542aded5f8571700ace4f1eb3f1591c29\nSuccessfully built efficientnet-pytorch\nInstalling collected packages: efficientnet-pytorch\nSuccessfully installed efficientnet-pytorch-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"MEAN = [0.9871, 0.9871, 0.9871]\nSTD = [0.9892, 0.9892, 0.9892]\n# TRANSFORMS = transforms.Compose([transforms.ToTensor(),\n#                                 transforms.Normalize(mean=MEAN, std=STD)])\nIMG_HEIGHT = 300\nIMG_WIDTH = 300\n\ndef remove_blobs(img, min_size=10, debug=False):\n    if debug:\n        fig, ax = plt.subplots(1,2, figsize=(30,8))\n        ax[0].imshow(img)\n        ax[0].set_title('original image', size=16)\n    \n    height, width = img.shape\n\n    # find all the connected components (white blobs in your image)\n    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(img, connectivity=8)\n    # Removes background, which is seen as a big component\n    sizes = stats[1:, -1]\n  \n    blob_idxs = []    \n    for idx, s in enumerate(sizes):\n        if s < min_size:\n            blob_idxs.append(idx+1)\n    \n    img[np.isin(output, blob_idxs)] = 0\n    \n    if debug:\n        ax[1].imshow(img)\n        ax[1].set_title('image with removed blobs', size=16)\n        plt.show()\n    \n    return img\n\n\ndef crop(img, debug=False):\n    if debug:\n        fig, ax = plt.subplots(1,2, figsize=(30,8))\n        ax[0].imshow(img)\n        ax[0].set_title(f'original image, shape: {img.shape}', size=16)\n        \n    _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n    \n    x_min, y_min, x_max, y_max = np.inf, np.inf, 0, 0\n    for cnt in contours:\n        x, y, w, h = cv2.boundingRect(cnt)\n        x_min = min(x_min, x)\n        y_min = min(y_min, y)\n        x_max = max(x_max, x + w)\n        y_max = max(y_max, y + h)\n\n    img_cropped = img[y_min:y_max, x_min:x_max]\n    \n    if debug:\n        ax[1].imshow(img_cropped)\n        ax[1].set_title(f'cropped image, shape: {img_cropped.shape}', size=16)\n        plt.show()\n    \n    return img_cropped\n\n\ndef pad_kernel(kernel, max_pad=np.inf):\n    kernel = np.array(kernel)\n    h, w = kernel.shape\n    pad_h = min((max(h, w) - h) // 2, max_pad)\n    pad_w = min((max(h, w) - w) // 2, max_pad)\n    return np.pad(kernel, ([pad_h, pad_h], [pad_w, pad_w]), 'constant', constant_values=-1)\n\n\n# creates a mask of missing pixels to be filled using\ndef create_mask(kernel, img_b):\n    mask = cv2.filter2D(img_b, -1, kernel)\n    kernel_flat_sum = (kernel == a).flatten().sum()\n    threshold_min = kernel_flat_sum * threshold_ratio\n    threshold_max = kernel_flat_sum + 1\n    return (mask > threshold_min) & (mask < threshold_max)\n\n\n# make kernels\na = np.float32(1.0 / 255.0)\nthreshold_ratio = 0.50\n# single pixel width horizontal line with 1 pixel missing\nkernel_h_single_mono = pad_kernel([\n    [ a, a,  a, -1,  a,  a, a ]\n], max_pad=1)\n# single pixel width horizontal line with 3 pixels missing\nkernel_h_single_triple = pad_kernel([\n    [ a, a, a, -1, -1, -1, a, a, a ]\n], max_pad=1)\n\nkernel_h_multi = pad_kernel([\n    [ a, a, a, a, a, a, a ],\n    [ a, a, a,-1, a, a, a ],\n    [ a, a, a, a, a, a, a ],\n], max_pad=1)\n\nkernel_v_single = pad_kernel([\n    [ a],\n    [ a],\n    [ a],\n    [-1],\n    [ a],\n    [ a],\n    [ a],\n], max_pad=1)\n\nkernel_v_multi = pad_kernel([\n    [ a, a, a ],\n    [ a, a, a ],\n    [ a, a, a ],\n    [ a,-1, a ],\n    [ a, a, a ],\n    [ a, a, a ],\n    [ a, a, a ],\n], max_pad=1)\n\nkernel_lr_single = pad_kernel([\n    [ -1,-1,-1,-1, a ],\n    [ -1,-1,-1, a,-1 ],\n    [ -1,-1,-1,-1,-1 ],\n    [ -1, a,-1,-1,-1 ],\n    [  a,-1,-1,-1,-1 ],\n])\n\nkernel_lr_multi = pad_kernel([\n    [ -1,-1,-1, a, a ],\n    [ -1,-1, a, a, a ],\n    [ -1, a,-1, a,-1 ],\n    [  a, a, a,-1,-1 ],\n    [  a, a,-1,-1,-1 ],\n])\n\nkernel_rl_single = pad_kernel([\n    [  a,-1,-1,-1,-1 ],\n    [ -1, a,-1,-1,-1 ],\n    [ -1,-1,-1,-1,-1 ],\n    [ -1,-1,-1, a,-1 ],\n    [ -1,-1,-1,-1, a ],\n])\n\nkernel_rl_multi = pad_kernel([\n    [ a, a,-1,-1,-1],\n    [ a, a, a,-1,-1],\n    [-1, a,-1, a,-1],\n    [-1,-1, a, a, a],\n    [-1,-1,-1, a, a],\n])\n\ndef fill_missing_pixels(img, debug):\n    img_b = img.astype(np.float32)\n    img_b[img_b > 0] = 255\n\n    mask_h_single_mono = create_mask(kernel_h_single_mono, img_b)\n\n    mask_h_single_triple = create_mask(kernel_h_single_triple, img_b)\n\n    mask_h_single = mask_h_single_mono | mask_h_single_triple\n\n    mask_h_multi = create_mask(kernel_h_multi, img_b)\n\n\n    mask_v_single = create_mask(kernel_v_single, img_b)\n\n\n    mask_v_multi = create_mask(kernel_v_multi, img_b)\n\n\n    mask_lr_single = create_mask(kernel_lr_single, img_b)\n\n\n    mask_lr_multi = create_mask(kernel_lr_multi, img_b)\n\n\n    mask_rl_single = create_mask(kernel_lr_single, img_b)\n\n\n    mask_rl_multi = create_mask(kernel_rl_multi, img_b)\n\n    mask_single = mask_h_single | mask_v_single | mask_lr_single | mask_rl_single\n    mask_multi = mask_h_multi  | mask_v_multi |mask_lr_multi | mask_rl_multi\n    mask = mask_single | mask_multi\n\n    if debug:\n        fig, ax = plt.subplots(2, 2 ,figsize=(35,20))\n        ax[0,0].imshow(mask_h_single)\n        ax[0,0].set_title('mask_h_single', size=16)\n        ax[0,1].imshow(mask_v_single)\n        ax[0,1].set_title('mask_v_single', size=16)\n        ax[1,0].imshow(mask_lr_single)\n        ax[1,0].set_title('mask_lr_single', size=16)\n        ax[1,1].imshow(mask_lr_single)\n        ax[1,1].set_title('mask_lr_single', size=16)\n        plt.show()\n\n        fig, ax = plt.subplots(2, 2, figsize=(35,20))\n        ax[0,0].imshow(mask_h_multi)\n        ax[0,0].set_title('mask_h_multi', size=16)\n        ax[0,1].imshow(mask_v_multi)\n        ax[0,1].set_title('mask_v_multi', size=16)\n        ax[1,0].imshow(mask_lr_multi)\n        ax[1,0].set_title('mask_lr_multi', size=16)\n        ax[1,1].imshow(mask_rl_multi)\n        ax[1,1].set_title('mask_rl_multi', size=16)\n        plt.show()\n\n        fig, ax = plt.subplots(2, 1 ,figsize=(15,20))\n        ax[0].imshow(img)\n        ax[0].set_title('original image', size=16)\n\n        img_fill = mask.copy()\n        img_fill[img_fill > 0] = 255\n\n        img_rgb = np.stack([\n            img_fill,\n            img_b,\n            np.zeros(img.shape),\n        ], axis=2)\n\n        ax[1].imshow(img_rgb)\n        ax[1].set_title('image with filled missing pixels (red)', size=16)\n        plt.show()    \n\n    # all pixels in the mask are filled up\n    img[mask] = 255\n\n    return img\n\n\ndef pad_resize(img):\n    h, w = img.shape\n    s = max(w, h)\n    pad_h, pad_v = 0, 0\n    hw_ratio = (h / w) - (IMG_HEIGHT / IMG_WIDTH)\n    if hw_ratio < 0:\n        pad_h = int(abs(hw_ratio) * w / 2)\n    else:\n        wh_ratio = (w / h) - (IMG_WIDTH / IMG_HEIGHT)\n        pad_v = int(abs(wh_ratio) * h // 2)\n\n    img = np.pad(img, [(pad_h, pad_h), (pad_v, pad_v)], mode='constant')\n    img = cv2.resize(img,(IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)\n\n    return img\n\n\ndef process_img(file_path, folder='train', debug=False):\n    # read image and invert colors to get black background and white moleculernir\n    img0 = 255 - cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n    \n    # rotate counter clockwise to get horizontal images\n    h, w = img0.shape\n    if h > w:\n        img0 = np.rot90(img0)\n    \n    # remove blobs, crop, fill missing pixels, pad and resize\n    img = remove_blobs(img0, min_size=2, debug=debug)\n    img = crop(img, debug=debug)\n    img = fill_missing_pixels(img, debug=debug)\n    img = pad_resize(img)\n    \n    if debug:\n        fig, ax = plt.subplots(1, 2, figsize=(20,10))\n        ax[0].imshow(img0)\n        ax[0].set_title('Original image', size=16)\n        ax[1].imshow(img)\n        ax[1].set_title('Fully processed image', size=16)\n    \n    # normalize to range 0-255 and encode as png\n    img = (img / img.max() * 255).astype(np.uint8)\n#     img = cv2.imencode('.png', img)[1].tobytes()\n#     print('img',img)\n\n    return img\n\n\nclass Tokenizer():\n    \n    def __init__(self, label_dir):\n        \n        self.prefix_list = ['c', 'h', 'b', 't', 'm', 's', 'i', 'h', 't', 'm', 's']\n        self.ELEM_REGEX = re.compile(r\"[A-Z][a-z]?[0-9]*\")\n        self.ATOM_REGEX = re.compile(r\"[A-Z][a-z]?\")\n        self.NUM_REGEX = re.compile(r\"[0-9]+|.\")\n        \n        self.max_str_len = 0\n        self.vocab = set()\n        self.label_dict = dict()\n        \n        with open(label_dir) as csvfile:\n            reader = csv.reader(csvfile)\n            next(reader, None)\n            for row in reader:\n                label = row[1].split('/', 1)[1]\n                tokenized_label = self._tokenize_InChI(label)\n                self.vocab.update(set(tokenized_label))\n                if self.max_str_len<len(tokenized_label):\n                    self.max_str_len=len(tokenized_label)\n        self.vocab = sorted(self.vocab)\n        self.stoi={v: idx+1 for idx,v in enumerate(self.vocab)}\n        self.itos={item[1]: item[0] for item in self.stoi.items()}\n        \n        with open(label_dir) as csvfile:\n            reader = csv.reader(csvfile)\n            next(reader, None)\n            for row in reader:\n                label = row[1].split('/', 1)[1]\n                tokenized_label = self._tokenize_InChI(label)\n                self.label_dict[row[0]] = [self.stoi[t] for t in tokenized_label]\n                \n        \n    def _tokenize_InChI(self, st):\n        st_list = st.split('/')\n        str_out = []\n        str_out.append('<sos>')\n        str_out.extend(self._tokenize_formula(st_list.pop(0)))\n        str_out.append('<eos>')\n        for prefix in self.prefix_list:\n            if st_list:\n                if st_list[0][0]==prefix[0]:\n                    str_out.extend(self.NUM_REGEX.findall(st_list.pop(0)[1:]))\n            str_out.append('<eos>')        \n        return str_out\n    \n\n    def _tokenize_formula(self, st):\n        st_out = []\n        elem_list = self.ELEM_REGEX.findall(st)\n        for elem in elem_list:\n            atom = self.ATOM_REGEX.findall(elem)[0]\n            st_out.append(atom)\n            count_str = elem[elem.find(atom)+len(atom):]\n            if count_str!='':\n                st_out.append(count_str)\n        return st_out","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def preprocess(img_dir, label_dir):\n    img_list = list()\n    for dirname, _, filenames in os.walk(img_dir):\n            for filename in filenames:\n                img_list.append(os.path.join(dirname, filename))\n    print('Image names loaded')\n        \n    tokenizer = Tokenizer(label_dir)\n    print('Labels loaded')\n    \n    return img_list, tokenizer\n\nimg_list, tokenizer = preprocess(img_dir ='../input/bms-molecular-translation/train', \n                                 label_dir='../input/bms-molecular-translation/train_labels.csv')\npickle.dump(img_list, open(\"train_image_names.p\", \"wb\"))\npickle.dump(tokenizer, open(\"train_label_data.p\", \"wb\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BMSdata (Dataset):\n    \n    def __init__(self, img_list, tokenizer, cleaning=None):\n        print('Intantiating dataset')\n        self.cleaning = cleaning\n        self.img_list = img_list\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.img_list)\n    \n    \n    def __getitem__(self, idx):\n        img = cv2.imread(self.img_list[idx])\n        #img = cv2.resize(img, (300, 300))\n        if self.cleaning:\n            img = self.cleaning(self.img_list[idx])\n            \n        img_id = self.img_list[idx].split('/')[-1][:-4]\n        label_ = np.array(self.tokenizer.label_dict[img_id])\n        len_ = label_.shape[0]\n            \n        src = np.zeros(self.tokenizer.max_str_len+1)\n        src[:len_] = label_\n            \n        trg = np.zeros(self.tokenizer.max_str_len+1)\n        trg[:len_-1] = label_[1:]\n            \n        return img, src, trg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_list = pickle.load(open(\"../input/processed-data/train_image_names.p\", \"rb\"))\ntokenizer = pickle.load(open(\"../input/processed-data/train_label_data.p\", \"rb\"))\ndataset = BMSdata(img_list, tokenizer, cleaning=process_img)\nint_to_char = np.array(list(dataset.tokenizer.itos.values()))\nint_to_char","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    def __init__(self, out_size):\n        super(Encoder, self).__init__()\n        self.image_encoder_model = EfficientNet.from_name('efficientnet-b0')\n        self.image_encoder_model._conv_stem = efficientnet_pytorch.utils.Conv2dStaticSamePadding(\n                                                1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False, image_size=302)\n        self.fc = nn.Linear(1000, out_size)\n        \n    def forward(self, input_):\n        conv_out = self.image_encoder_model(input_).squeeze()\n        enc_out = self.fc(conv_out)\n        return enc_out\n\n    \ndef generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\nclass PositionalEncoding(nn.Module):\n    \n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\nclass Decoder(nn.Module):\n\n    def __init__(self, in_size, nhead, hidden_size, nlayers, out_size, dropout=0):\n        super(Decoder, self).__init__()\n        \n        self.pos_encoder = PositionalEncoding(in_size, dropout)\n        decoder_layers = TransformerDecoderLayer(in_size, nhead, hidden_size, dropout)\n        self.transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n        self.linear = nn.Linear(in_size, out_size)\n\n    def forward(self, tgt, memory, tgt_mask):\n        tgt = self.pos_encoder(tgt)\n        output = self.transformer_decoder(tgt, memory, tgt_mask)\n        edge_logit = self.linear(output)\n        return edge_logit\n\n\ndef _inflate(tensor, times, dim):\n    # repeat_dims = [1] * tensor.dim()\n    # repeat_dims[dim] = times\n    # return tensor.repeat(*repeat_dims)\n    return torch.repeat_interleave(tensor, times, dim)\n\n\nclass TopKDecoder(torch.nn.Module):\n    r\"\"\"\n    Top-K decoding with beam search.\n\n    Args:\n        decoder_rnn (DecoderRNN): An object of DecoderRNN used for decoding.\n        k (int): Size of the beam.\n\n    Inputs: inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio\n        - **inputs** (seq_len, batch, input_size): list of sequences, whose length is the batch size and within which\n          each sequence is a list of token IDs.  It is used for teacher forcing when provided. (default is `None`)\n        - **encoder_hidden** (num_layers * num_directions, batch_size, hidden_size): tensor containing the features\n          in the hidden state `h` of encoder. Used as the initial hidden state of the decoder.\n        - **encoder_outputs** (batch, seq_len, hidden_size): tensor with containing the outputs of the encoder.\n          Used for attention mechanism (default is `None`).\n        - **function** (torch.nn.Module): A function used to generate symbols from RNN hidden state\n          (default is `torch.nn.functional.log_softmax`).\n        - **teacher_forcing_ratio** (float): The probability that teacher forcing will be used. A random number is\n          drawn uniformly from 0-1 for every decoding token, and if the sample is smaller than the given value,\n          teacher forcing would be used (default is 0).\n\n    Outputs: decoder_outputs, decoder_hidden, ret_dict\n        - **decoder_outputs** (batch): batch-length list of tensors with size (max_length, hidden_size) containing the\n          outputs of the decoder.\n        - **decoder_hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the last hidden\n          state of the decoder.\n        - **ret_dict**: dictionary containing additional information as follows {*length* : list of integers\n          representing lengths of output sequences, *topk_length*: list of integers representing lengths of beam search\n          sequences, *sequence* : list of sequences, where each sequence is a list of predicted token IDs,\n          *topk_sequence* : list of beam search sequences, each beam is a list of token IDs, *inputs* : target\n          outputs if provided for decoding}.\n    \"\"\"\n\n    def __init__(self, decoder_rnn, k, decoder_dim, max_length, tokenizer):\n        super(TopKDecoder, self).__init__()\n        self.rnn = decoder_rnn\n        self.k = k\n        self.hidden_size = decoder_dim  # self.rnn.hidden_size\n        self.V = len(tokenizer)\n        self.SOS = tokenizer.stoi[\"<sos>\"]\n        self.EOS = tokenizer.stoi[\"<eos>\"]\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n\n    def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None, function=F.log_softmax,\n                teacher_forcing_ratio=0, retain_output_probs=True):\n        \"\"\"\n        Forward rnn for MAX_LENGTH steps.  Look at :func:`seq2seq.models.DecoderRNN.DecoderRNN.forward_rnn` for details.\n        \"\"\"\n\n        # inputs, batch_size, max_length = self.rnn._validate_args(inputs, encoder_hidden, encoder_outputs,\n        #                                                         function, teacher_forcing_ratio)\n\n        batch_size = encoder_outputs.size(0)\n        max_length = self.max_length\n\n        self.pos_index = (torch.LongTensor(range(batch_size)) * self.k).view(-1, 1).cuda()\n\n        # Inflate the initial hidden states to be of size: b*k x h\n        # encoder_hidden = self.rnn._init_state(encoder_hidden)\n        if encoder_hidden is None:\n            hidden = None\n        else:\n            if isinstance(encoder_hidden, tuple):\n                # hidden = tuple([_inflate(h, self.k, 1) for h in encoder_hidden])\n                hidden = tuple([h.squeeze(0) for h in encoder_hidden])\n                hidden = tuple([_inflate(h, self.k, 0) for h in hidden])\n                hidden = tuple([h.unsqueeze(0) for h in hidden])\n            else:\n                # hidden = _inflate(encoder_hidden, self.k, 1)\n                raise RuntimeError(\"Not supported\")\n\n        # ... same idea for encoder_outputs and decoder_outputs\n        if True:  # self.rnn.use_attention:\n            inflated_encoder_outputs = _inflate(encoder_outputs, self.k, 0)\n        else:\n            inflated_encoder_outputs = None\n\n        # Initialize the scores; for the first step,\n        # ignore the inflated copies to avoid duplicate entries in the top k\n        sequence_scores = torch.Tensor(batch_size * self.k, 1)\n        sequence_scores.fill_(-float('Inf'))\n        sequence_scores.index_fill_(0, torch.LongTensor([i * self.k for i in range(0, batch_size)]), 0.0)\n        sequence_scores = sequence_scores.cuda()\n\n        # Initialize the input vector\n        input_var = torch.transpose(torch.LongTensor([[self.SOS] * batch_size * self.k]), 0, 1).cuda()\n\n        # Store decisions for backtracking\n        stored_outputs = list()\n        stored_scores = list()\n        stored_predecessors = list()\n        stored_emitted_symbols = list()\n        stored_hidden = list()\n\n        for i in range(0, max_length):\n\n            # Run the RNN one step forward\n            log_softmax_output, hidden, _ = self.rnn.forward_step(input_var, hidden,\n                                                                  inflated_encoder_outputs, function=function)\n            # If doing local backprop (e.g. supervised training), retain the output layer\n            if retain_output_probs:\n                stored_outputs.append(log_softmax_output)\n\n            # To get the full sequence scores for the new candidates, add the local scores for t_i to the predecessor scores for t_(i-1)\n            sequence_scores = _inflate(sequence_scores, self.V, 1)\n            sequence_scores += log_softmax_output.squeeze(1)\n            scores, candidates = sequence_scores.view(batch_size, -1).topk(self.k, dim=1)\n\n            # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n            input_var = (candidates % self.V).view(batch_size * self.k, 1)\n            sequence_scores = scores.view(batch_size * self.k, 1)\n\n            # Update fields for next timestep\n            predecessors = (candidates // self.V + self.pos_index.expand_as(candidates)).view(batch_size * self.k, 1)\n            if isinstance(hidden, tuple):\n                hidden = tuple([h.index_select(1, predecessors.squeeze()) for h in hidden])\n            else:\n                hidden = hidden.index_select(1, predecessors.squeeze())\n\n            # Update sequence scores and erase scores for end-of-sentence symbol so that they aren't expanded\n            stored_scores.append(sequence_scores.clone())\n            eos_indices = input_var.data.eq(self.EOS)\n            if eos_indices.nonzero().dim() > 0:\n                sequence_scores.data.masked_fill_(eos_indices, -float('inf'))\n\n            # Cache results for backtracking\n            stored_predecessors.append(predecessors)\n            stored_emitted_symbols.append(input_var)\n            stored_hidden.append(hidden)\n\n        # Do backtracking to return the optimal values\n        output, h_t, h_n, s, l, p = self._backtrack(stored_outputs, stored_hidden,\n                                                    stored_predecessors, stored_emitted_symbols,\n                                                    stored_scores, batch_size, self.hidden_size)\n\n        # Build return objects\n        decoder_outputs = [step[:, 0, :] for step in output]\n        if isinstance(h_n, tuple):\n            decoder_hidden = tuple([h[:, :, 0, :] for h in h_n])\n        else:\n            decoder_hidden = h_n[:, :, 0, :]\n        metadata = {}\n        metadata['inputs'] = inputs\n        metadata['output'] = output\n        metadata['h_t'] = h_t\n        metadata['score'] = s\n        metadata['topk_length'] = l\n        metadata['topk_sequence'] = p\n        metadata['length'] = [seq_len[0] for seq_len in l]\n        metadata['sequence'] = [seq[0] for seq in p]\n        return decoder_outputs, decoder_hidden, metadata\n\n    def _backtrack(self, nw_output, nw_hidden, predecessors, symbols, scores, b, hidden_size):\n        \"\"\"Backtracks over batch to generate optimal k-sequences.\n\n        Args:\n            nw_output [(batch*k, vocab_size)] * sequence_length: A Tensor of outputs from network\n            nw_hidden [(num_layers, batch*k, hidden_size)] * sequence_length: A Tensor of hidden states from network\n            predecessors [(batch*k)] * sequence_length: A Tensor of predecessors\n            symbols [(batch*k)] * sequence_length: A Tensor of predicted tokens\n            scores [(batch*k)] * sequence_length: A Tensor containing sequence scores for every token t = [0, ... , seq_len - 1]\n            b: Size of the batch\n            hidden_size: Size of the hidden state\n\n        Returns:\n            output [(batch, k, vocab_size)] * sequence_length: A list of the output probabilities (p_n)\n            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n\n            h_t [(batch, k, hidden_size)] * sequence_length: A list containing the output features (h_n)\n            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n\n            h_n(batch, k, hidden_size): A Tensor containing the last hidden state for all top-k sequences.\n\n            score [batch, k]: A list containing the final scores for all top-k sequences\n\n            length [batch, k]: A list specifying the length of each sequence in the top-k candidates\n\n            p (batch, k, sequence_len): A Tensor containing predicted sequence\n        \"\"\"\n\n        lstm = isinstance(nw_hidden[0], tuple)\n\n        # initialize return variables given different types\n        output = list()\n        h_t = list()\n        p = list()\n        # Placeholder for last hidden state of top-k sequences.\n        # If a (top-k) sequence ends early in decoding, `h_n` contains\n        # its hidden state when it sees EOS.  Otherwise, `h_n` contains\n        # the last hidden state of decoding.\n        if lstm:\n            state_size = nw_hidden[0][0].size()\n            h_n = tuple([torch.zeros(state_size).cuda(), torch.zeros(state_size).cuda()])\n        else:\n            h_n = torch.zeros(nw_hidden[0].size()).cuda()\n        l = [[self.max_length] * self.k for _ in range(b)]  # Placeholder for lengths of top-k sequences\n        # Similar to `h_n`\n\n        # the last step output of the beams are not sorted\n        # thus they are sorted here\n        sorted_score, sorted_idx = scores[-1].view(b, self.k).topk(self.k)\n        # initialize the sequence scores with the sorted last step beam scores\n        s = sorted_score.clone()\n\n        batch_eos_found = [0] * b  # the number of EOS found\n        # in the backward loop below for each batch\n\n        t = self.max_length - 1\n        # initialize the back pointer with the sorted order of the last step beams.\n        # add self.pos_index for indexing variable with b*k as the first dimension.\n        t_predecessors = (sorted_idx + self.pos_index.expand_as(sorted_idx)).view(b * self.k)\n        while t >= 0:\n            # Re-order the variables with the back pointer\n            current_output = nw_output[t].index_select(0, t_predecessors)\n            if lstm:\n                current_hidden = tuple([h.index_select(1, t_predecessors) for h in nw_hidden[t]])\n            else:\n                current_hidden = nw_hidden[t].index_select(1, t_predecessors)\n            current_symbol = symbols[t].index_select(0, t_predecessors)\n            # Re-order the back pointer of the previous step with the back pointer of\n            # the current step\n            t_predecessors = predecessors[t].index_select(0, t_predecessors).squeeze()\n\n            # This tricky block handles dropped sequences that see EOS earlier.\n            # The basic idea is summarized below:\n            #\n            #   Terms:\n            #       Ended sequences = sequences that see EOS early and dropped\n            #       Survived sequences = sequences in the last step of the beams\n            #\n            #       Although the ended sequences are dropped during decoding,\n            #   their generated symbols and complete backtracking information are still\n            #   in the backtracking variables.\n            #   For each batch, everytime we see an EOS in the backtracking process,\n            #       1. If there is survived sequences in the return variables, replace\n            #       the one with the lowest survived sequence score with the new ended\n            #       sequences\n            #       2. Otherwise, replace the ended sequence with the lowest sequence\n            #       score with the new ended sequence\n            #\n            eos_indices = symbols[t].data.squeeze(1).eq(self.EOS).nonzero()\n            if eos_indices.dim() > 0:\n                for i in range(eos_indices.size(0) - 1, -1, -1):\n                    # Indices of the EOS symbol for both variables\n                    # with b*k as the first dimension, and b, k for\n                    # the first two dimensions\n                    idx = eos_indices[i]\n                    b_idx = int(idx[0] // self.k)\n                    # The indices of the replacing position\n                    # according to the replacement strategy noted above\n                    res_k_idx = self.k - (batch_eos_found[b_idx] % self.k) - 1\n                    batch_eos_found[b_idx] += 1\n                    res_idx = b_idx * self.k + res_k_idx\n\n                    # Replace the old information in return variables\n                    # with the new ended sequence information\n                    t_predecessors[res_idx] = predecessors[t][idx[0]]\n                    current_output[res_idx, :] = nw_output[t][idx[0], :]\n                    if lstm:\n                        current_hidden[0][:, res_idx, :] = nw_hidden[t][0][:, idx[0], :]\n                        current_hidden[1][:, res_idx, :] = nw_hidden[t][1][:, idx[0], :]\n                        h_n[0][:, res_idx, :] = nw_hidden[t][0][:, idx[0], :].data\n                        h_n[1][:, res_idx, :] = nw_hidden[t][1][:, idx[0], :].data\n                    else:\n                        current_hidden[:, res_idx, :] = nw_hidden[t][:, idx[0], :]\n                        h_n[:, res_idx, :] = nw_hidden[t][:, idx[0], :].data\n                    current_symbol[res_idx, :] = symbols[t][idx[0]]\n                    s[b_idx, res_k_idx] = scores[t][idx[0]].data[0]\n                    l[b_idx][res_k_idx] = t + 1\n\n            # record the back tracked results\n            output.append(current_output)\n            h_t.append(current_hidden)\n            p.append(current_symbol)\n\n            t -= 1\n\n        # Sort and re-order again as the added ended sequences may change\n        # the order (very unlikely)\n        s, re_sorted_idx = s.topk(self.k)\n        for b_idx in range(b):\n            l[b_idx] = [l[b_idx][k_idx.item()] for k_idx in re_sorted_idx[b_idx, :]]\n\n        re_sorted_idx = (re_sorted_idx + self.pos_index.expand_as(re_sorted_idx)).view(b * self.k)\n\n        # Reverse the sequences and re-order at the same time\n        # It is reversed because the backtracking happens in reverse time order\n        output = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(output)]\n        p = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(p)]\n        if lstm:\n            h_t = [tuple([h.index_select(1, re_sorted_idx).view(-1, b, self.k, hidden_size) for h in step]) for step in reversed(h_t)]\n            h_n = tuple([h.index_select(1, re_sorted_idx.data).view(-1, b, self.k, hidden_size) for h in h_n])\n        else:\n            h_t = [step.index_select(1, re_sorted_idx).view(-1, b, self.k, hidden_size) for step in reversed(h_t)]\n            h_n = h_n.index_select(1, re_sorted_idx.data).view(-1, b, self.k, hidden_size)\n        s = s.data\n\n        return output, h_t, h_n, s, l, p\n\n    def _mask_symbol_scores(self, score, idx, masking_score=-float('inf')):\n        score[idx] = masking_score\n\n    def _mask(self, tensor, idx, dim=0, masking_score=-float('inf')):\n        if len(idx.size()) > 0:\n            indices = idx[:, 0]\n            tensor.index_fill_(dim, indices, masking_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate model\n#out_channel_size = [32, 64, 128, 256, 512, 1024]\n#in_channels = 3\nencoder = Encoder(out_size=128).cuda()\ndecoder = Decoder(in_size=128, nhead=4, hidden_size=256, nlayers=6, out_size=185).cuda()\nembedding = nn.Embedding(185, 128, padding_idx=0).cuda()\nmodel = encoder, decoder, embedding","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\n\ndef train(model, dataset):\n    \n    total_steps = 60000\n    warmup_steps = 500\n    n = dataset.__len__()\n    num_eval = int(0.2*n)\n    num_train =  n-num_eval\n    \n    encoder, decoder, embedding = model\n    parameters = list(encoder.parameters())\n    parameters.extend(list(decoder.parameters()))\n    parameters.extend(list(embedding.parameters()))\n        \n    max_val_acc = 0\n    softmax_ = nn.Softmax(2)\n    criterion = nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n    optimizer = torch.optim.Adam(parameters, lr=0.005, weight_decay=0)\n    scheduler = transformers.get_cosine_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n    encoder.train()\n    decoder.train()\n    embedding.train()\n    \n    step=0\n    epoch=0\n    ld_list = []\n    loss_list = []\n    lr_list = []\n    while step<total_steps:\n        train_dataset, val_dataset = random_split(dataset, [num_train, num_eval])\n        train_loader = DataLoader(dataset, batch_size=64, shuffle=True, sampler=None, batch_sampler=None)\n        #val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True, sampler=None, batch_sampler=None)\n        \n        for img, src, trg in train_loader:\n            step+=1\n            img = img.unsqueeze(1).float().cuda()\n            trg_mask = generate_square_subsequent_mask(286).cuda()\n            src = src.long().cuda()\n            src_emb = embedding(src).permute(1, 0, 2)\n            trg = trg.long().cuda()-1\n            encoder_out = encoder(img)\n            decoder_out = decoder(src_emb, encoder_out, trg_mask)\n            #print(encoder_out.shape, src.shape, decoder_out.shape, trg.shape)\n            \n            loss = torch.mean(criterion(decoder_out.permute(1,2,0), trg))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            if step%100==0:\n                ld = get_LD(softmax_(decoder_out), trg)\n                ld_list.append(ld)\n                loss_ = float(loss.cpu().detach().numpy())\n                loss_list.append(loss_)\n                lr_ = get_lr(optimizer)\n                lr_list.append(lr_)\n                print('Step: {} | LD: {:.4f} | Loss: {:.4f} | learning rate: {:.6f}'.format(step, ld, loss_, lr_))\n                # save data after each epoch\n                torch.save(encoder.state_dict(), '/kaggle/working/encoder_best_model.ckpt')\n                torch.save(decoder.state_dict(), '/kaggle/working/decoder_best_model.ckpt')\n                torch.save(embedding.state_dict(), '/kaggle/working/embedding_best_model.ckpt')                \n                pickle.dump(ld_list, open('/kaggle/working/ld_list.p','wb'))\n                pickle.dump(loss_list, open('/kaggle/working/loss_list.p','wb'))\n                pickle.dump(lr_list, open('/kaggle/working/lr_list.p','wb'))\n            \n\n\ndef get_LD(decoder_softmax, target):\n    decoder_logit = torch.argmax(decoder_softmax, 2)\n    decoder_logit = decoder_logit.detach().cpu().numpy()\n    target = target.permute(1,0).detach().cpu().numpy()\n    \n    acc = []\n    for i in range(decoder_logit.shape[1]):\n        output_ = ''.join(int_to_char[decoder_logit[:,i]]).replace('<eos>', '/')\n        target_ = ''.join(int_to_char[target[:,i]]).replace('<eos>', '/')\n        \n        try:\n            eos_d = [m.start() for m in re.finditer(r'/', output_)][-1]\n            output_ = output_[:eos_d]\n        except IndexError:\n            pass\n        \n        try:\n            eos_t = [m.start() for m in re.finditer(r'/', target_)][-1]\n            target_ = target[:eos_t]\n        except IndexError:\n            pass\n\n        print(output_)\n        print(target_)\n        acc.append(levenshtein_distance(output_, target_))\n    \n    return sum(acc)/len(acc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\ntrain(model, dataset)","metadata":{},"execution_count":null,"outputs":[]}]}